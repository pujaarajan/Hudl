{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!JAVA_OPTS=\"$JAVA_OPTS -Xmx=8g -Xms=8\"\n",
    "\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "sc.stop()\n",
    "conf = (SparkConf()\n",
    " .set(\"spark.driver.memory\", \"8g\")\n",
    " .set(\"spark.driver.maxResultSize\", \"8g\")\n",
    " .set(\"spark.executor.memory\", \"10000mb\")\n",
    " .set(\"spark.storage.memoryFraction\", \"0\")\n",
    " .set(\"spark.executor.instances\", \"100\"))\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from ds_utils.pipeline import client, mongo\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def highlight_properties(x):\n",
    "    if 'ai' in x and len(x['ai']) > 0:\n",
    "        sport = x['ai'][0]['st']\n",
    "        location = x['ai'][0]['sl']\n",
    "        team = x['ai'][0]['t']\n",
    "    else:\n",
    "        sport = None\n",
    "        location = None\n",
    "        team = None\n",
    "    highlight_sport = (x['_id'], sport, location, team)\n",
    "    return highlight_sport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sec(s):\n",
    "    L = s.split(':')\n",
    "    if len(L) == 1:\n",
    "        return L[0]\n",
    "    elif len(L) == 2:\n",
    "        datee = datetime.datetime.strptime(s, \"%M:%S\")\n",
    "        return datee.minute * 60 + datee.second\n",
    "    elif len(L) == 3:\n",
    "        datee = datetime.datetime.strptime(s, \"%H:%M:%S\")\n",
    "        return datee.hour * 3600 + datee.minute * 60 + datee.second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def duration(x):\n",
    "    s = 0\n",
    "    if 'c' in x and len(x['c'])>0:\n",
    "        for clip in x['c']:\n",
    "            if type(clip['d']) == int:\n",
    "                s = s + (int(clip['d'])/10000000)\n",
    "            elif type(clip['d']) == unicode and len(clip['d']) == 8:\n",
    "                s = s + get_sec(clip['d'])\n",
    "            elif type(clip['d']) == unicode and len(clip['d']) == 16:\n",
    "                s = s + get_sec(clip['d'][0:8])\n",
    "            else:\n",
    "                continue\n",
    "    else:\n",
    "        s = None\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recommendations = sc.textFile(\"s3://ds-fulla/mongo/20150507/monolith/highlights/userhighlights/\")\\\n",
    "    .map(json.loads)\\\n",
    "    .filter(lambda x: x['p'] and 'hm' in x and 'xt' in x)\\\n",
    "    .filter(lambda x: 'ai' in x and len(x['ai']) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format(x):\n",
    "    loc = x[0][2]\n",
    "    if ',' in loc:\n",
    "        s = loc.split(', ')\n",
    "        city = s[0]\n",
    "        state = s[1]\n",
    "    elif loc != '':\n",
    "        city = loc\n",
    "        state = ''\n",
    "    else:\n",
    "        city = ''\n",
    "        state = ''\n",
    "    d = {'id': x[0][0], 'sport' : x[0][1], 'city' : city, 'state' : state, 'team' : x[0][3], 'duration' : x[1]}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recommendation2 = recommendations.map(lambda x: (highlight_properties(x), duration(x)))\\\n",
    "    .map(format)\n",
    "# ID, Sport, Location, Team, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177935"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendation2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH = os.path.join(\"s3://hudl-hadoop/ds-highlight-recommendations\", 'temp_date', \"intermediate_data/\")\n",
    "ALL_SPORTS_PATH = os.path.join(PATH, 'all_sports/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If failes, call:\n",
    "# s3cmd rm -r s3://hudl-hadoop/ds-highlight-recommendations/\n",
    "recommendation2 \\\n",
    "    .map(json.dumps) \\\n",
    "    .saveAsTextFile(ALL_SPORTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recommendation3 = sc.textFile(ALL_SPORTS_PATH).map(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sport = {\n",
    "    1:  'football',\n",
    "    2:  'basketball',\n",
    "    3:  'wrestling',\n",
    "    4:  'volleyball',\n",
    "    5:  'baseball',\n",
    "    6:  'soccer',\n",
    "    7:  'lacrosse',\n",
    "    8:  'golf',\n",
    "    9:  'gymnastics',\n",
    "    10: 'softball',\n",
    "    11: 'swimdive',\n",
    "    12: 'trackfield',\n",
    "    13: 'icehockey',\n",
    "    14: 'fieldhockey',\n",
    "    15: 'waterpolo',\n",
    "    16: 'cheerspirit',\n",
    "    17: 'dancedrill',\n",
    "    18: 'cricket',\n",
    "    19: 'crosscountry',\n",
    "    20: 'performingarts',\n",
    "    21: 'rugby',\n",
    "    22: 'tennis',\n",
    "    100: 'other'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for s in sport:\n",
    "    recommendation4 = recommendation3.filter(lambda x: x['sport'] == s)\n",
    "    \n",
    "    SPORT_PATH = os.path.join(PATH, 'sports/', sport[s])\n",
    "    \n",
    "    recommendation4.map(json.dumps) \\\n",
    "        .saveAsTextFile(SPORT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lon_lat_range():\n",
    "    min_lon = sqlCtx.sql(\"select min(double(longitude)) as min_lon from hudl_sql.schools\") \\\n",
    "        .collect()[0] \\\n",
    "        .min_lon\n",
    "    max_lon = sqlCtx.sql(\"select max(double(longitude)) as max_lon from hudl_sql.schools\") \\\n",
    "        .collect()[0] \\\n",
    "        .max_lon\n",
    "    min_lat = sqlCtx.sql(\"select min(double(latitude)) as min_lat from hudl_sql.schools\") \\\n",
    "        .collect()[0] \\\n",
    "        .min_lat\n",
    "    max_lat = sqlCtx.sql(\"select max(double(latitude)) as max_lat from hudl_sql.schools\") \\\n",
    "        .collect()[0] \\\n",
    "        .max_lat\n",
    "    return min_lon, max_lon, min_lat, max_lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o379.javaToPython.\n: java.lang.OutOfMemoryError: PermGen space\n\tat java.lang.ClassLoader.defineClass1(Native Method)\n\tat java.lang.ClassLoader.defineClass(ClassLoader.java:800)\n\tat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n\tat java.net.URLClassLoader.defineClass(URLClassLoader.java:449)\n\tat java.net.URLClassLoader.access$100(URLClassLoader.java:71)\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:361)\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n\tat org.apache.spark.sql.execution.Aggregate.output(Aggregate.scala:61)\n\tat org.apache.spark.sql.execution.Aggregate$$anonfun$1$$anonfun$apply$1.applyOrElse(Aggregate.scala:82)\n\tat org.apache.spark.sql.execution.Aggregate$$anonfun$1$$anonfun$apply$1.applyOrElse(Aggregate.scala:78)\n\tat scala.PartialFunction$Lifted.apply(PartialFunction.scala:218)\n\tat scala.PartialFunction$Lifted.apply(PartialFunction.scala:214)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$collect$1.apply(TreeNode.scala:129)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$collect$1.apply(TreeNode.scala:129)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreach(TreeNode.scala:88)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreach$1.apply(TreeNode.scala:89)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreach$1.apply(TreeNode.scala:89)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreach(TreeNode.scala:89)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.collect(TreeNode.scala:129)\n\tat org.apache.spark.sql.execution.Aggregate$$anonfun$1.apply(Aggregate.scala:78)\n\tat org.apache.spark.sql.execution.Aggregate$$anonfun$1.apply(Aggregate.scala:77)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-48e170baebfd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmin_lon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_lon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_lat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_lat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlon_lat_range\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m location_query = \"\"\"\n\u001b[0;32m      4\u001b[0m \u001b[0mselect\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mteamid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdouble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlongitude\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlongitude\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-51-fa7ce081b092>\u001b[0m in \u001b[0;36mlon_lat_range\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlon_lat_range\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmin_lon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqlCtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"select min(double(longitude)) as min_lon from hudl_sql.schools\"\u001b[0m\u001b[1;33m)\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mmin_lon\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mmax_lon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqlCtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"select max(double(longitude)) as max_lon from hudl_sql.schools\"\u001b[0m\u001b[1;33m)\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mmax_lon\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmin_lat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqlCtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"select min(double(latitude)) as min_lat from hudl_sql.schools\"\u001b[0m\u001b[1;33m)\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mmin_lat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmax_lat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqlCtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"select max(double(latitude)) as max_lat from hudl_sql.schools\"\u001b[0m\u001b[1;33m)\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mmax_lat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/hadoop/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \"\"\"\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjavaToPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m         \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_create_cls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/hadoop/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/hadoop/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o379.javaToPython.\n: java.lang.OutOfMemoryError: PermGen space\n\tat java.lang.ClassLoader.defineClass1(Native Method)\n\tat java.lang.ClassLoader.defineClass(ClassLoader.java:800)\n\tat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n\tat java.net.URLClassLoader.defineClass(URLClassLoader.java:449)\n\tat java.net.URLClassLoader.access$100(URLClassLoader.java:71)\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:361)\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n\tat org.apache.spark.sql.execution.Aggregate.output(Aggregate.scala:61)\n\tat org.apache.spark.sql.execution.Aggregate$$anonfun$1$$anonfun$apply$1.applyOrElse(Aggregate.scala:82)\n\tat org.apache.spark.sql.execution.Aggregate$$anonfun$1$$anonfun$apply$1.applyOrElse(Aggregate.scala:78)\n\tat scala.PartialFunction$Lifted.apply(PartialFunction.scala:218)\n\tat scala.PartialFunction$Lifted.apply(PartialFunction.scala:214)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$collect$1.apply(TreeNode.scala:129)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$collect$1.apply(TreeNode.scala:129)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreach(TreeNode.scala:88)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreach$1.apply(TreeNode.scala:89)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreach$1.apply(TreeNode.scala:89)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreach(TreeNode.scala:89)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.collect(TreeNode.scala:129)\n\tat org.apache.spark.sql.execution.Aggregate$$anonfun$1.apply(Aggregate.scala:78)\n\tat org.apache.spark.sql.execution.Aggregate$$anonfun$1.apply(Aggregate.scala:77)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)\n"
     ]
    }
   ],
   "source": [
    "min_lon, max_lon, min_lat, max_lat = lon_lat_range()\n",
    "\n",
    "location_query = \"\"\"\n",
    "select t.teamid,\n",
    "    double(s.longitude) as longitude,\n",
    "    double(s.latitude) as latitude\n",
    "from hudl_sql.teams t\n",
    "inner join schools s\n",
    "on t.schoolid=s.schoolid\n",
    "\"\"\"\n",
    "\n",
    "location_vector_rdd = sqlCtx.sql(location_query) \\\n",
    "    .map(\n",
    "        # (user_id, np.array([normalized_lon, normalized_lat]))\n",
    "        lambda x: (\n",
    "            x.teamid,\n",
    "            np.array([\n",
    "                (x.longitude - min_lon) / (max_lon - min_lon),\n",
    "                (x.latitude - min_lat) / (max_lat - min_lat)\n",
    "            ])\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "location_vector_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_group(x):\n",
    "    if x[3] != \"\":\n",
    "        x[\"group\"] = x[\"state\"]\n",
    "    else:\n",
    "        x[\"group\"] = \"OTHER\"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SPORT = 1;\n",
    "recommendation5 = sc.textFile(os.path.join(PATH, 'sports/', sport[SPORT]))\\\n",
    "    .map(json.loads)\\\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recommendation5.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ".map(assign_group).cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
